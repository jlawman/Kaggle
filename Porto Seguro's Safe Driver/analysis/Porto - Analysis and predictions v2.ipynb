{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set target, id and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "892816"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_id=train['id']\n",
    "train_target = train['target']\n",
    "train = train.drop(['id','target'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id=test['id']\n",
    "test = test.drop('id',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal with missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace -1 values with nan\n",
    "From description: \"Values of -1 indicate that the feature was missing from the observation.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.replace(to_replace=-1,value=np.nan)\n",
    "test = test.replace(to_replace=-1,value=np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing data 1: drop columns with 20%+ missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Columns whereGreater than 20% missing\n",
    "incomplete_columns = list(train.isnull().sum()[train.isnull().sum()>len(train)*.2].index)\n",
    "\n",
    "train = train.drop(incomplete_columns,axis=1)\n",
    "test = test.drop(incomplete_columns, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing data 2: impute median values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.fillna(train.median())\n",
    "test = test.fillna(test.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert categorical features to binary values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_feat = [col for col in train.columns if col.endswith('cat')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for df in [train,test]:\n",
    "    for column in cat_feat:\n",
    "        dummies = pd.get_dummies(df[column],drop_first=True)\n",
    "        df = pd.concat([df,dummies],axis=1)\n",
    "        df = df.drop([column],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()   #Initalize scaler estimator\n",
    "\n",
    "scaler.fit(train) #Remember to only fit scaler to training datab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.DataFrame(scaler.transform(train),columns=train.columns)\n",
    "test = pd.DataFrame(scaler.transform(test),columns=test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=0.95, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=.95)\n",
    "\n",
    "pca.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(595212, 55)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca_train = pca.transform(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not going with PCA at this time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_valid, y_train, y_valid = train_test_split(train, train_target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grad = GradientBoostingClassifier(verbose=2,n_estimators=500,max_depth=6,max_features=5,\n",
    "                                  min_samples_leaf=50,min_samples_split=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.3118           13.70m\n",
      "         2           0.3111           12.21m\n",
      "         3           0.3104           13.34m\n",
      "         4           0.3097           13.58m\n",
      "         5           0.3092           13.29m\n",
      "         6           0.3086           13.05m\n",
      "         7           0.3082           13.10m\n",
      "         8           0.3077           13.31m\n",
      "         9           0.3072           13.37m\n",
      "        10           0.3068           13.42m\n",
      "        11           0.3064           13.21m\n",
      "        12           0.3061           13.13m\n",
      "        13           0.3057           12.96m\n",
      "        14           0.3055           12.74m\n",
      "        15           0.3052           12.57m\n",
      "        16           0.3050           12.39m\n",
      "        17           0.3048           12.22m\n",
      "        18           0.3046           12.17m\n",
      "        19           0.3044           12.00m\n",
      "        20           0.3041           11.91m\n",
      "        21           0.3039           11.88m\n",
      "        22           0.3037           11.87m\n",
      "        23           0.3036           11.77m\n",
      "        24           0.3034           11.71m\n",
      "        25           0.3032           11.65m\n",
      "        26           0.3030           11.57m\n",
      "        27           0.3029           11.53m\n",
      "        28           0.3027           11.48m\n",
      "        29           0.3026           11.37m\n",
      "        30           0.3024           11.31m\n",
      "        31           0.3023           11.24m\n",
      "        32           0.3022           11.17m\n",
      "        33           0.3020           11.10m\n",
      "        34           0.3019           11.05m\n",
      "        35           0.3018           11.01m\n",
      "        36           0.3017           10.96m\n",
      "        37           0.3016           10.92m\n",
      "        38           0.3015           10.83m\n",
      "        39           0.3014           10.75m\n",
      "        40           0.3013           10.71m\n",
      "        41           0.3012           10.65m\n",
      "        42           0.3011           10.57m\n",
      "        43           0.3009           10.52m\n",
      "        44           0.3009           10.45m\n",
      "        45           0.3008           10.37m\n",
      "        46           0.3007           10.32m\n",
      "        47           0.3006           10.23m\n",
      "        48           0.3006           10.15m\n",
      "        49           0.3005           10.08m\n",
      "        50           0.3004           10.03m\n",
      "        51           0.3003            9.98m\n",
      "        52           0.3002            9.95m\n",
      "        53           0.3001            9.92m\n",
      "        54           0.3000            9.89m\n",
      "        55           0.3000            9.82m\n",
      "        56           0.2999            9.78m\n",
      "        57           0.2998            9.74m\n",
      "        58           0.2997            9.67m\n",
      "        59           0.2996            9.63m\n",
      "        60           0.2995            9.60m\n",
      "        61           0.2994            9.56m\n",
      "        62           0.2993            9.52m\n",
      "        63           0.2993            9.46m\n",
      "        64           0.2992            9.40m\n",
      "        65           0.2991            9.35m\n",
      "        66           0.2991            9.31m\n",
      "        67           0.2990            9.28m\n",
      "        68           0.2989            9.25m\n",
      "        69           0.2988            9.21m\n",
      "        70           0.2987            9.15m\n",
      "        71           0.2987            9.11m\n",
      "        72           0.2986            9.09m\n",
      "        73           0.2985            9.05m\n",
      "        74           0.2984            9.03m\n",
      "        75           0.2984            8.97m\n",
      "        76           0.2983            8.94m\n",
      "        77           0.2982            8.91m\n",
      "        78           0.2982            8.88m\n",
      "        79           0.2981            8.84m\n",
      "        80           0.2980            8.81m\n",
      "        81           0.2980            8.78m\n",
      "        82           0.2979            8.73m\n",
      "        83           0.2978            8.71m\n",
      "        84           0.2978            8.66m\n",
      "        85           0.2977            8.61m\n",
      "        86           0.2976            8.58m\n",
      "        87           0.2976            8.54m\n",
      "        88           0.2975            8.52m\n",
      "        89           0.2974            8.49m\n",
      "        90           0.2973            8.47m\n",
      "        91           0.2973            8.44m\n",
      "        92           0.2972            8.41m\n",
      "        93           0.2972            8.37m\n",
      "        94           0.2971            8.35m\n",
      "        95           0.2970            8.34m\n",
      "        96           0.2970            8.35m\n",
      "        97           0.2969            8.33m\n",
      "        98           0.2968            8.29m\n",
      "        99           0.2968            8.24m\n",
      "       100           0.2967            8.22m\n",
      "       101           0.2966            8.20m\n",
      "       102           0.2965            8.17m\n",
      "       103           0.2965            8.14m\n",
      "       104           0.2964            8.13m\n",
      "       105           0.2963            8.11m\n",
      "       106           0.2963            8.07m\n",
      "       107           0.2962            8.04m\n",
      "       108           0.2961            8.02m\n",
      "       109           0.2961            7.99m\n",
      "       110           0.2960            7.97m\n",
      "       111           0.2960            7.93m\n",
      "       112           0.2959            7.89m\n",
      "       113           0.2959            7.87m\n",
      "       114           0.2958            7.84m\n",
      "       115           0.2958            7.80m\n",
      "       116           0.2957            7.77m\n",
      "       117           0.2956            7.74m\n",
      "       118           0.2956            7.71m\n",
      "       119           0.2956            7.66m\n",
      "       120           0.2955            7.63m\n",
      "       121           0.2955            7.60m\n",
      "       122           0.2954            7.58m\n",
      "       123           0.2953            7.56m\n",
      "       124           0.2953            7.53m\n",
      "       125           0.2952            7.49m\n",
      "       126           0.2952            7.46m\n",
      "       127           0.2952            7.43m\n",
      "       128           0.2951            7.39m\n",
      "       129           0.2951            7.37m\n",
      "       130           0.2950            7.33m\n",
      "       131           0.2950            7.30m\n",
      "       132           0.2949            7.28m\n",
      "       133           0.2949            7.24m\n",
      "       134           0.2948            7.22m\n",
      "       135           0.2947            7.20m\n",
      "       136           0.2947            7.17m\n",
      "       137           0.2946            7.15m\n",
      "       138           0.2945            7.12m\n",
      "       139           0.2945            7.08m\n",
      "       140           0.2945            7.06m\n",
      "       141           0.2944            7.02m\n",
      "       142           0.2943            7.01m\n",
      "       143           0.2943            6.98m\n",
      "       144           0.2942            6.96m\n",
      "       145           0.2941            6.94m\n",
      "       146           0.2941            6.92m\n",
      "       147           0.2940            6.90m\n",
      "       148           0.2940            6.87m\n",
      "       149           0.2939            6.84m\n",
      "       150           0.2939            6.81m\n",
      "       151           0.2939            6.77m\n",
      "       152           0.2938            6.73m\n",
      "       153           0.2938            6.71m\n",
      "       154           0.2937            6.69m\n",
      "       155           0.2936            6.67m\n",
      "       156           0.2936            6.65m\n",
      "       157           0.2935            6.62m\n",
      "       158           0.2935            6.60m\n",
      "       159           0.2934            6.57m\n",
      "       160           0.2934            6.55m\n",
      "       161           0.2933            6.52m\n",
      "       162           0.2933            6.50m\n",
      "       163           0.2932            6.47m\n",
      "       164           0.2931            6.46m\n",
      "       165           0.2931            6.43m\n",
      "       166           0.2931            6.40m\n",
      "       167           0.2931            6.36m\n",
      "       168           0.2930            6.33m\n",
      "       169           0.2930            6.31m\n",
      "       170           0.2929            6.29m\n",
      "       171           0.2928            6.27m\n",
      "       172           0.2928            6.26m\n",
      "       173           0.2927            6.23m\n",
      "       174           0.2927            6.21m\n",
      "       175           0.2926            6.20m\n",
      "       176           0.2925            6.18m\n",
      "       177           0.2925            6.16m\n",
      "       178           0.2924            6.14m\n",
      "       179           0.2924            6.11m\n",
      "       180           0.2923            6.10m\n",
      "       181           0.2922            6.08m\n",
      "       182           0.2922            6.05m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       183           0.2921            6.03m\n",
      "       184           0.2921            6.02m\n",
      "       185           0.2921            5.99m\n",
      "       186           0.2920            5.96m\n",
      "       187           0.2920            5.94m\n",
      "       188           0.2919            5.92m\n",
      "       189           0.2918            5.90m\n",
      "       190           0.2918            5.88m\n",
      "       191           0.2918            5.85m\n",
      "       192           0.2917            5.83m\n",
      "       193           0.2917            5.81m\n",
      "       194           0.2916            5.79m\n",
      "       195           0.2916            5.77m\n",
      "       196           0.2915            5.74m\n",
      "       197           0.2915            5.72m\n",
      "       198           0.2914            5.69m\n",
      "       199           0.2914            5.68m\n",
      "       200           0.2913            5.66m\n",
      "       201           0.2912            5.64m\n",
      "       202           0.2912            5.62m\n",
      "       203           0.2911            5.60m\n",
      "       204           0.2911            5.58m\n",
      "       205           0.2910            5.56m\n",
      "       206           0.2910            5.53m\n",
      "       207           0.2909            5.52m\n",
      "       208           0.2909            5.50m\n",
      "       209           0.2908            5.48m\n",
      "       210           0.2907            5.46m\n",
      "       211           0.2907            5.44m\n",
      "       212           0.2907            5.41m\n",
      "       213           0.2906            5.39m\n",
      "       214           0.2906            5.37m\n",
      "       215           0.2905            5.35m\n",
      "       216           0.2904            5.33m\n",
      "       217           0.2904            5.31m\n",
      "       218           0.2903            5.29m\n",
      "       219           0.2903            5.27m\n",
      "       220           0.2902            5.25m\n",
      "       221           0.2902            5.23m\n",
      "       222           0.2901            5.21m\n",
      "       223           0.2901            5.19m\n",
      "       224           0.2900            5.17m\n",
      "       225           0.2900            5.15m\n",
      "       226           0.2899            5.13m\n",
      "       227           0.2899            5.12m\n",
      "       228           0.2899            5.09m\n",
      "       229           0.2898            5.07m\n",
      "       230           0.2897            5.05m\n",
      "       231           0.2897            5.03m\n",
      "       232           0.2896            5.01m\n",
      "       233           0.2896            4.99m\n",
      "       234           0.2895            4.97m\n",
      "       235           0.2895            4.95m\n",
      "       236           0.2894            4.93m\n",
      "       237           0.2893            4.92m\n",
      "       238           0.2893            4.89m\n",
      "       239           0.2893            4.87m\n",
      "       240           0.2892            4.85m\n",
      "       241           0.2892            4.83m\n",
      "       242           0.2892            4.80m\n",
      "       243           0.2891            4.78m\n",
      "       244           0.2891            4.76m\n",
      "       245           0.2891            4.73m\n",
      "       246           0.2890            4.71m\n",
      "       247           0.2890            4.69m\n",
      "       248           0.2889            4.67m\n",
      "       249           0.2889            4.64m\n",
      "       250           0.2889            4.62m\n",
      "       251           0.2889            4.60m\n",
      "       252           0.2888            4.58m\n",
      "       253           0.2887            4.56m\n",
      "       254           0.2887            4.54m\n",
      "       255           0.2887            4.52m\n",
      "       256           0.2886            4.49m\n",
      "       257           0.2886            4.47m\n",
      "       258           0.2886            4.45m\n",
      "       259           0.2885            4.44m\n",
      "       260           0.2884            4.42m\n",
      "       261           0.2884            4.40m\n",
      "       262           0.2883            4.38m\n",
      "       263           0.2883            4.37m\n",
      "       264           0.2883            4.34m\n",
      "       265           0.2882            4.32m\n",
      "       266           0.2882            4.30m\n",
      "       267           0.2881            4.29m\n",
      "       268           0.2881            4.27m\n",
      "       269           0.2880            4.25m\n",
      "       270           0.2880            4.23m\n",
      "       271           0.2880            4.21m\n",
      "       272           0.2879            4.19m\n",
      "       273           0.2879            4.17m\n",
      "       274           0.2878            4.16m\n",
      "       275           0.2877            4.14m\n",
      "       276           0.2877            4.12m\n",
      "       277           0.2877            4.11m\n",
      "       278           0.2876            4.09m\n",
      "       279           0.2876            4.07m\n",
      "       280           0.2876            4.05m\n",
      "       281           0.2875            4.04m\n",
      "       282           0.2875            4.02m\n",
      "       283           0.2875            4.00m\n",
      "       284           0.2874            3.99m\n",
      "       285           0.2874            3.97m\n",
      "       286           0.2873            3.95m\n",
      "       287           0.2873            3.93m\n",
      "       288           0.2872            3.91m\n",
      "       289           0.2872            3.90m\n",
      "       290           0.2871            3.88m\n",
      "       291           0.2871            3.86m\n",
      "       292           0.2871            3.84m\n",
      "       293           0.2870            3.81m\n",
      "       294           0.2870            3.80m\n",
      "       295           0.2870            3.78m\n",
      "       296           0.2869            3.76m\n",
      "       297           0.2869            3.74m\n",
      "       298           0.2868            3.72m\n",
      "       299           0.2868            3.70m\n",
      "       300           0.2867            3.68m\n",
      "       301           0.2867            3.66m\n",
      "       302           0.2866            3.65m\n",
      "       303           0.2866            3.63m\n",
      "       304           0.2865            3.61m\n",
      "       305           0.2865            3.59m\n",
      "       306           0.2864            3.57m\n",
      "       307           0.2864            3.55m\n",
      "       308           0.2863            3.53m\n",
      "       309           0.2863            3.51m\n",
      "       310           0.2863            3.49m\n",
      "       311           0.2862            3.47m\n",
      "       312           0.2862            3.45m\n",
      "       313           0.2862            3.43m\n",
      "       314           0.2861            3.41m\n",
      "       315           0.2861            3.39m\n",
      "       316           0.2861            3.37m\n",
      "       317           0.2860            3.35m\n",
      "       318           0.2860            3.34m\n",
      "       319           0.2859            3.32m\n",
      "       320           0.2859            3.30m\n",
      "       321           0.2859            3.28m\n",
      "       322           0.2858            3.26m\n",
      "       323           0.2857            3.24m\n",
      "       324           0.2857            3.22m\n",
      "       325           0.2857            3.20m\n",
      "       326           0.2856            3.18m\n",
      "       327           0.2856            3.16m\n",
      "       328           0.2855            3.15m\n",
      "       329           0.2855            3.13m\n",
      "       330           0.2854            3.11m\n",
      "       331           0.2853            3.09m\n",
      "       332           0.2853            3.07m\n",
      "       333           0.2853            3.05m\n",
      "       334           0.2852            3.03m\n",
      "       335           0.2852            3.02m\n",
      "       336           0.2851            3.00m\n",
      "       337           0.2850            2.98m\n",
      "       338           0.2850            2.96m\n",
      "       339           0.2850            2.94m\n",
      "       340           0.2849            2.92m\n",
      "       341           0.2849            2.90m\n",
      "       342           0.2849            2.88m\n",
      "       343           0.2848            2.87m\n",
      "       344           0.2848            2.85m\n",
      "       345           0.2847            2.83m\n",
      "       346           0.2847            2.81m\n",
      "       347           0.2846            2.79m\n",
      "       348           0.2846            2.78m\n",
      "       349           0.2846            2.76m\n",
      "       350           0.2845            2.74m\n",
      "       351           0.2844            2.72m\n",
      "       352           0.2844            2.70m\n",
      "       353           0.2843            2.68m\n",
      "       354           0.2843            2.67m\n",
      "       355           0.2842            2.65m\n",
      "       356           0.2842            2.63m\n",
      "       357           0.2841            2.61m\n",
      "       358           0.2841            2.59m\n",
      "       359           0.2841            2.57m\n",
      "       360           0.2840            2.55m\n",
      "       361           0.2840            2.54m\n",
      "       362           0.2840            2.52m\n",
      "       363           0.2839            2.50m\n",
      "       364           0.2839            2.48m\n",
      "       365           0.2838            2.46m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       366           0.2838            2.45m\n",
      "       367           0.2837            2.43m\n",
      "       368           0.2837            2.41m\n",
      "       369           0.2837            2.39m\n",
      "       370           0.2836            2.37m\n",
      "       371           0.2835            2.35m\n",
      "       372           0.2835            2.33m\n",
      "       373           0.2835            2.32m\n",
      "       374           0.2834            2.30m\n",
      "       375           0.2834            2.28m\n",
      "       376           0.2833            2.26m\n",
      "       377           0.2833            2.24m\n",
      "       378           0.2833            2.23m\n",
      "       379           0.2832            2.21m\n",
      "       380           0.2831            2.19m\n",
      "       381           0.2831            2.17m\n",
      "       382           0.2830            2.16m\n",
      "       383           0.2830            2.14m\n",
      "       384           0.2829            2.12m\n",
      "       385           0.2829            2.10m\n",
      "       386           0.2828            2.08m\n",
      "       387           0.2828            2.07m\n",
      "       388           0.2827            2.05m\n",
      "       389           0.2827            2.03m\n",
      "       390           0.2826            2.01m\n",
      "       391           0.2826            1.99m\n",
      "       392           0.2826            1.97m\n",
      "       393           0.2825            1.96m\n",
      "       394           0.2825            1.94m\n",
      "       395           0.2825            1.92m\n",
      "       396           0.2824            1.90m\n",
      "       397           0.2824            1.88m\n",
      "       398           0.2824            1.86m\n",
      "       399           0.2823            1.84m\n",
      "       400           0.2822            1.83m\n",
      "       401           0.2822            1.81m\n",
      "       402           0.2821            1.79m\n",
      "       403           0.2821            1.77m\n",
      "       404           0.2820            1.75m\n",
      "       405           0.2820            1.74m\n",
      "       406           0.2820            1.72m\n",
      "       407           0.2819            1.70m\n",
      "       408           0.2819            1.68m\n",
      "       409           0.2818            1.66m\n",
      "       410           0.2818            1.65m\n",
      "       411           0.2817            1.63m\n",
      "       412           0.2817            1.61m\n",
      "       413           0.2816            1.59m\n",
      "       414           0.2816            1.57m\n",
      "       415           0.2816            1.55m\n",
      "       416           0.2815            1.53m\n",
      "       417           0.2815            1.52m\n",
      "       418           0.2814            1.50m\n",
      "       419           0.2814            1.48m\n",
      "       420           0.2813            1.46m\n",
      "       421           0.2813            1.44m\n",
      "       422           0.2812            1.42m\n",
      "       423           0.2812            1.41m\n",
      "       424           0.2811            1.39m\n",
      "       425           0.2811            1.37m\n",
      "       426           0.2811            1.35m\n",
      "       427           0.2810            1.33m\n",
      "       428           0.2810            1.31m\n",
      "       429           0.2809            1.30m\n",
      "       430           0.2809            1.28m\n",
      "       431           0.2808            1.26m\n",
      "       432           0.2808            1.24m\n",
      "       433           0.2807            1.22m\n",
      "       434           0.2807            1.21m\n",
      "       435           0.2806            1.19m\n",
      "       436           0.2806            1.17m\n",
      "       437           0.2805            1.15m\n",
      "       438           0.2805            1.13m\n",
      "       439           0.2805            1.11m\n",
      "       440           0.2804            1.10m\n",
      "       441           0.2804            1.08m\n",
      "       442           0.2803            1.06m\n",
      "       443           0.2802            1.04m\n",
      "       444           0.2802            1.02m\n",
      "       445           0.2801            1.01m\n",
      "       446           0.2801           59.27s\n",
      "       447           0.2800           58.18s\n",
      "       448           0.2800           57.06s\n",
      "       449           0.2799           55.97s\n",
      "       450           0.2799           54.87s\n",
      "       451           0.2799           53.77s\n",
      "       452           0.2798           52.66s\n",
      "       453           0.2798           51.57s\n",
      "       454           0.2797           50.50s\n",
      "       455           0.2797           49.38s\n",
      "       456           0.2796           48.28s\n",
      "       457           0.2796           47.17s\n",
      "       458           0.2796           46.07s\n",
      "       459           0.2795           44.98s\n",
      "       460           0.2795           43.89s\n",
      "       461           0.2794           42.81s\n",
      "       462           0.2794           41.69s\n",
      "       463           0.2793           40.60s\n",
      "       464           0.2793           39.51s\n",
      "       465           0.2792           38.41s\n",
      "       466           0.2792           37.30s\n",
      "       467           0.2792           36.20s\n",
      "       468           0.2791           35.10s\n",
      "       469           0.2790           34.02s\n",
      "       470           0.2790           32.92s\n",
      "       471           0.2789           31.82s\n",
      "       472           0.2789           30.73s\n",
      "       473           0.2788           29.64s\n",
      "       474           0.2788           28.55s\n",
      "       475           0.2788           27.45s\n",
      "       476           0.2787           26.36s\n",
      "       477           0.2787           25.26s\n",
      "       478           0.2786           24.16s\n",
      "       479           0.2786           23.06s\n",
      "       480           0.2786           21.95s\n",
      "       481           0.2785           20.86s\n",
      "       482           0.2785           19.76s\n",
      "       483           0.2785           18.65s\n",
      "       484           0.2784           17.56s\n",
      "       485           0.2784           16.46s\n",
      "       486           0.2783           15.37s\n",
      "       487           0.2783           14.27s\n",
      "       488           0.2782           13.17s\n",
      "       489           0.2781           12.08s\n",
      "       490           0.2781           10.98s\n",
      "       491           0.2780            9.88s\n",
      "       492           0.2780            8.79s\n",
      "       493           0.2779            7.69s\n",
      "       494           0.2779            6.59s\n",
      "       495           0.2779            5.49s\n",
      "       496           0.2778            4.39s\n",
      "       497           0.2778            3.29s\n",
      "       498           0.2777            2.20s\n",
      "       499           0.2777            1.10s\n",
      "       500           0.2776            0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=6,\n",
       "              max_features=5, max_leaf_nodes=None,\n",
       "              min_impurity_split=1e-07, min_samples_leaf=50,\n",
       "              min_samples_split=70, min_weight_fraction_leaf=0.0,\n",
       "              n_estimators=500, presort='auto', random_state=None,\n",
       "              subsample=1.0, verbose=2, warm_start=False)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad.fit(train,train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(892816, 2)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = grad.predict_proba(test)\n",
    "\n",
    "grad_pred = pred[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random search for best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\"learning_rate\":[0.1,0.2,0.3,0.4],\n",
    "\n",
    "#param_dist = {\"max_depth\": [3,5,10],\n",
    " #             \"max_features\":[2,4,6,8],          \n",
    "#             \"min_samples_split\":[2,50,100]}\n",
    "\n",
    "param_dist = {\"max_depth\": [6,7],\n",
    "              \"max_features\":[4,5,6],\n",
    "              \"min_samples_leaf\":[1,10,30,50,100],\n",
    "             \"min_samples_split\":[40,50,60,70]}\n",
    "\n",
    "best_params = {\n",
    "    \"max_depth\":6,\n",
    "    \"max_features\":5,\n",
    "    \"min_samples_leaf\":50,\n",
    "    \"min_samples_split\":70\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rand_search = RandomizedSearchCV(grad,param_dist,cv=3,verbose=1,n_iter=3,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "      Iter       Train Loss   Remaining Time \n",
      "      Iter       Train Loss   Remaining Time \n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.3116           25.34s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.3116           30.24s\n",
      "         1           0.3117           28.73s\n",
      "         1           0.3118           20.95s\n",
      "         2           0.3105           22.57s\n",
      "         2           0.3106           25.59s\n",
      "         2           0.3111           17.38s\n",
      "         2           0.3106           24.09s\n",
      "         3           0.3097           19.85s\n",
      "         3           0.3102           15.76s\n",
      "         3           0.3098           22.10s\n",
      "         3           0.3098           20.84s\n",
      "         4           0.3090           16.93s\n",
      "         4           0.3096           13.75s\n",
      "         4           0.3090           18.57s\n",
      "         4           0.3091           17.60s\n",
      "         5           0.3082           14.08s\n",
      "         5           0.3089           12.69s\n",
      "         5           0.3082           15.49s\n",
      "         5           0.3084           14.99s\n",
      "         6           0.3077           11.56s\n",
      "         6           0.3082           10.10s\n",
      "         6           0.3078           12.00s\n",
      "         6           0.3076           12.61s\n",
      "         7           0.3078            7.60s\n",
      "         7           0.3071            8.93s\n",
      "         7           0.3072            8.92s\n",
      "         7           0.3070            9.34s\n",
      "         8           0.3073            5.01s\n",
      "         8           0.3066            5.82s\n",
      "         8           0.3066            6.02s\n",
      "         8           0.3065            6.26s\n",
      "         9           0.3068            2.56s\n",
      "         9           0.3061            2.94s\n",
      "        10           0.3063            0.00s\n",
      "         9           0.3061            3.01s\n",
      "         9           0.3060            3.11s\n",
      "        10           0.3057            0.00s\n",
      "        10           0.3055            0.00s\n",
      "        10           0.3056            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.3119           22.67s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.3119           20.54s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         2           0.3110           19.98s\n",
      "         1           0.3121           22.88s\n",
      "         2           0.3111           19.54s\n",
      "         1           0.3121           22.68s\n",
      "         3           0.3101           16.32s\n",
      "         3           0.3101           16.47s\n",
      "         2           0.3112           22.32s\n",
      "         2           0.3111           22.64s\n",
      "         4           0.3094           14.41s\n",
      "         4           0.3095           13.63s\n",
      "         3           0.3104           19.34s\n",
      "         3           0.3102           18.81s\n",
      "         5           0.3087           12.15s\n",
      "         5           0.3087           11.75s\n",
      "         4           0.3096           17.28s\n",
      "         6           0.3082            9.89s\n",
      "         4           0.3094           16.92s\n",
      "         6           0.3081            9.56s\n",
      "         7           0.3077            7.33s\n",
      "         5           0.3089           14.39s\n",
      "         7           0.3076            7.04s\n",
      "         5           0.3088           14.00s\n",
      "         8           0.3072            4.83s\n",
      "         6           0.3084           11.25s\n",
      "         8           0.3071            4.68s\n",
      "         6           0.3083           11.18s\n",
      "         9           0.3067            2.40s\n",
      "         9           0.3065            2.32s\n",
      "         7           0.3079            8.44s\n",
      "         7           0.3078            8.68s\n",
      "        10           0.3062            0.00s\n",
      "        10           0.3061            0.00s\n",
      "         8           0.3074            5.69s\n",
      "         8           0.3074            5.67s\n",
      "         9           0.3069            2.66s\n",
      "         9           0.3070            2.74s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "        10           0.3065            0.00s\n",
      "        10           0.3066            0.00s\n",
      "         1           0.3120           19.85s\n",
      "         2           0.3111           13.48s\n",
      "         3           0.3102           11.77s\n",
      "         4           0.3095            9.11s\n",
      "         5           0.3089            6.91s\n",
      "         6           0.3083            5.38s\n",
      "         7           0.3077            3.93s\n",
      "         8           0.3073            2.61s\n",
      "         9           0.3069            1.32s\n",
      "        10           0.3065            0.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:  1.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.3120           10.68s\n",
      "         2           0.3112            9.85s\n",
      "         3           0.3106            8.53s\n",
      "         4           0.3098            7.44s\n",
      "         5           0.3091            6.06s\n",
      "         6           0.3085            4.92s\n",
      "         7           0.3080            3.74s\n",
      "         8           0.3075            2.67s\n",
      "         9           0.3072            1.36s\n",
      "        10           0.3068            0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise',\n",
       "          estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "              min_samples_split=20, min_weight_fraction_leaf=0.0,\n",
       "              n_estimators=10, presort='auto', random_state=None,\n",
       "              subsample=1.0, verbose=5, warm_start=False),\n",
       "          fit_params={}, iid=True, n_iter=3, n_jobs=-1,\n",
       "          param_distributions={'max_depth': [4, 5, 6], 'max_features': [5, 6, 7], 'min_samples_leaf': [1, 10, 50, 100, 1000], 'min_samples_split': [40, 50, 60, 70]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score=True, scoring=None, verbose=1)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_search.fit(train,train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_pred = rand_search.best_estimator_.predict_proba(test)\n",
    "\n",
    "best_pred = best_pred[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96355248214081701"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'friedman_mse',\n",
       " 'init': None,\n",
       " 'learning_rate': 0.1,\n",
       " 'loss': 'deviance',\n",
       " 'max_depth': 6,\n",
       " 'max_features': 5,\n",
       " 'max_leaf_nodes': None,\n",
       " 'min_impurity_split': 1e-07,\n",
       " 'min_samples_leaf': 50,\n",
       " 'min_samples_split': 70,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 10,\n",
       " 'presort': 'auto',\n",
       " 'random_state': None,\n",
       " 'subsample': 1.0,\n",
       " 'verbose': 5,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_search.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create output csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame({'id':test_id,'target':grad_pred})\n",
    "\n",
    "sub.to_csv('Grad Boost 9.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
